diff --git a/modeling/aspp.py b/modeling/aspp.py
index 5a97879..770e60f 100644
--- a/modeling/aspp.py
+++ b/modeling/aspp.py
@@ -68,7 +68,7 @@ class ASPP(nn.Module):
         x3 = self.aspp3(x)
         x4 = self.aspp4(x)
         x5 = self.global_avg_pool(x)
-        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)
+        x5 = F.interpolate(x5, size=x4.size()[2:], mode='nearest', align_corners=None)
         x = torch.cat((x1, x2, x3, x4, x5), dim=1)
 
         x = self.conv1(x)
diff --git a/modeling/backbone/mobilenet.py b/modeling/backbone/mobilenet.py
index 6fff541..9edce54 100644
--- a/modeling/backbone/mobilenet.py
+++ b/modeling/backbone/mobilenet.py
@@ -5,22 +5,21 @@ import math
 from modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d
 import torch.utils.model_zoo as model_zoo
 
+from aimet_torch.defs import PassThroughOp
 def conv_bn(inp, oup, stride, BatchNorm):
     return nn.Sequential(
         nn.Conv2d(inp, oup, 3, stride, 1, bias=False),
         BatchNorm(oup),
         nn.ReLU6(inplace=True)
     )
-
-
-def fixed_padding(inputs, kernel_size, dilation):
-    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)
-    pad_total = kernel_size_effective - 1
-    pad_beg = pad_total // 2
-    pad_end = pad_total - pad_beg
-    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))
-    return padded_inputs
-
+def _make_divisible(v, divisor=8, min_value=None):
+    if min_value is None:
+        min_value = divisor
+    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
+    # Make sure that round down does not go down by more than 10%.
+    if new_v < 0.9 * v:
+        new_v += divisor
+    return new_v
 
 class InvertedResidual(nn.Module):
     def __init__(self, inp, oup, stride, dilation, expand_ratio, BatchNorm):
@@ -33,10 +32,15 @@ class InvertedResidual(nn.Module):
         self.kernel_size = 3
         self.dilation = dilation
 
+        # More generally: padding = (ks // 2) * dilation for odd kernel sizes. ks is fixed to 3,
+        # ks // 2 == 1, so (ks // 2) * dilation = dilation
+        padding = dilation
         if expand_ratio == 1:
             self.conv = nn.Sequential(
                 # dw
-                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 0, dilation, groups=hidden_dim, bias=False),
+                nn.Conv2d(
+                    hidden_dim, hidden_dim, 3, stride,
+                    padding, dilation, groups=hidden_dim, bias=False),
                 BatchNorm(hidden_dim),
                 nn.ReLU6(inplace=True),
                 # pw-linear
@@ -46,11 +50,13 @@ class InvertedResidual(nn.Module):
         else:
             self.conv = nn.Sequential(
                 # pw
-                nn.Conv2d(inp, hidden_dim, 1, 1, 0, 1, bias=False),
+                # It is stupid to pad here, but we need it for backwards compatibility
+                nn.Conv2d(inp, hidden_dim, 1, 1, padding, 1, bias=False),
                 BatchNorm(hidden_dim),
                 nn.ReLU6(inplace=True),
                 # dw
-                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 0, dilation, groups=hidden_dim, bias=False),
+                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 0, dilation,
+                          groups=hidden_dim, bias=False),
                 BatchNorm(hidden_dim),
                 nn.ReLU6(inplace=True),
                 # pw-linear
@@ -59,14 +65,12 @@ class InvertedResidual(nn.Module):
             )
 
     def forward(self, x):
-        x_pad = fixed_padding(x, self.kernel_size, dilation=self.dilation)
         if self.use_res_connect:
-            x = x + self.conv(x_pad)
+            x = x + self.conv(x)
         else:
-            x = self.conv(x_pad)
+            x = self.conv(x)
         return x
 
-
 class MobileNetV2(nn.Module):
     def __init__(self, output_stride=8, BatchNorm=None, width_mult=1., pretrained=True):
         super(MobileNetV2, self).__init__()
@@ -87,7 +91,8 @@ class MobileNetV2(nn.Module):
 
         # building first layer
         input_channel = int(input_channel * width_mult)
-        self.features = [conv_bn(3, input_channel, 2, BatchNorm)]
+        # self.features = [conv_bn(3, input_channel, 2, BatchNorm)]
+        features = [conv_bn(3, input_channel, 2, BatchNorm)]
         current_stride *= 2
         # building inverted residual blocks
         for t, c, n, s in interverted_residual_setting:
@@ -102,18 +107,24 @@ class MobileNetV2(nn.Module):
             output_channel = int(c * width_mult)
             for i in range(n):
                 if i == 0:
-                    self.features.append(block(input_channel, output_channel, stride, dilation, t, BatchNorm))
+                    features.append(block(input_channel, output_channel, stride, dilation, t, BatchNorm))
+                    # self.features.append(block(input_channel, output_channel, stride, dilation, t, BatchNorm))
                 else:
-                    self.features.append(block(input_channel, output_channel, 1, dilation, t, BatchNorm))
+                    features.append(block(input_channel, output_channel, 1, dilation, t, BatchNorm))
+                    # self.features.append(block(input_channel, output_channel, 1, dilation, t, BatchNorm))
+                    
                 input_channel = output_channel
-        self.features = nn.Sequential(*self.features)
+        # self.features = nn.Sequential(*self.features)
         self._initialize_weights()
 
+        
+        # self.low_level_features = self.features[0:4]
+        # self.high_level_features = self.features[4:]
         if pretrained:
             self._load_pretrained_model()
+        self.low_level_features = nn.Sequential(*features[0:4])
 
-        self.low_level_features = self.features[0:4]
-        self.high_level_features = self.features[4:]
+        self.high_level_features = nn.Sequential(*features[4:])
 
     def forward(self, x):
         low_level_feat = self.low_level_features(x)
@@ -141,8 +152,8 @@ class MobileNetV2(nn.Module):
                 m.bias.data.zero_()
             elif isinstance(m, nn.BatchNorm2d):
                 m.weight.data.fill_(1)
-                m.bias.data.zero_()
-
+                m.bias.data.zero_() 
+                
 if __name__ == "__main__":
     input = torch.rand(1, 3, 512, 512)
     model = MobileNetV2(output_stride=16, BatchNorm=nn.BatchNorm2d)
diff --git a/modeling/decoder.py b/modeling/decoder.py
index 5ed41d0..ec4485e 100644
--- a/modeling/decoder.py
+++ b/modeling/decoder.py
@@ -36,7 +36,7 @@ class Decoder(nn.Module):
         low_level_feat = self.bn1(low_level_feat)
         low_level_feat = self.relu(low_level_feat)
 
-        x = F.interpolate(x, size=low_level_feat.size()[2:], mode='bilinear', align_corners=True)
+        x = F.interpolate(x, size=low_level_feat.size()[2:], mode='nearest', align_corners=None)
         x = torch.cat((x, low_level_feat), dim=1)
         x = self.last_conv(x)
 
diff --git a/modeling/deeplab.py b/modeling/deeplab.py
index 91907f8..8308934 100644
--- a/modeling/deeplab.py
+++ b/modeling/deeplab.py
@@ -28,7 +28,7 @@ class DeepLab(nn.Module):
         x, low_level_feat = self.backbone(input)
         x = self.aspp(x)
         x = self.decoder(x, low_level_feat)
-        x = F.interpolate(x, size=input.size()[2:], mode='bilinear', align_corners=True)
+        x = F.interpolate(x, size=input.size()[2:], mode='nearest', align_corners=None)
 
         return x
 
