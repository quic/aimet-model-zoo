{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6189f92",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9682e",
   "metadata": {},
   "source": [
    "Uncomment and run this block if you see an error `No module named \"utils\" found`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181df87",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from aimet_torch.quantsim import QuantizationSimModel\n",
    "from aimet_torch.qc_quantize_op import QuantScheme\n",
    "from utils.imresize import imresize\n",
    "import utils.blocks as blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad0177",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Global Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd1f05",
   "metadata": {},
   "source": [
    "These are all the constant variables that we use throughout this notebook. It also specifies the different model variations that were trained. You can select the necessary model at the end of this notebook while running inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b3209",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RGB_WEIGHTS = torch.FloatTensor([65.481, 128.553, 24.966])\n",
    "\n",
    "MODEL_ARGS = {\n",
    "    'ABPNRelease': {\n",
    "        'abpn_28_2x': {\n",
    "            'num_channels': 28,\n",
    "            'scaling_factor': 2,\n",
    "        },\n",
    "        'abpn_28_3x': {\n",
    "            'num_channels': 28,\n",
    "            'scaling_factor': 3,\n",
    "        },\n",
    "        'abpn_28_4x': {\n",
    "            'num_channels': 28,\n",
    "            'scaling_factor': 4,\n",
    "        },\n",
    "        'abpn_32_2x': {\n",
    "            'num_channels': 32,\n",
    "            'scaling_factor': 2,\n",
    "        },\n",
    "        'abpn_32_3x': {\n",
    "            'num_channels': 32,\n",
    "            'scaling_factor': 3,\n",
    "        },\n",
    "        'abpn_32_4x': {\n",
    "            'num_channels': 32,\n",
    "            'scaling_factor': 4,\n",
    "        }\n",
    "    },\n",
    "    'XLSRRelease': {\n",
    "        'xlsr_2x': {\n",
    "            'scaling_factor': 2,\n",
    "        },\n",
    "        'xlsr_3x': {\n",
    "            'scaling_factor': 3,\n",
    "        },\n",
    "        'xlsr_4x': {\n",
    "            'scaling_factor': 4,\n",
    "        }\n",
    "    },\n",
    "    'SESRRelease_M3': {\n",
    "        'sesr_m3_2x': {\n",
    "            'scaling_factor': 2\n",
    "        },\n",
    "        'sesr_m3_3x': {\n",
    "            'scaling_factor': 3\n",
    "        },\n",
    "        'sesr_m3_4x': {\n",
    "            'scaling_factor': 4\n",
    "        },\n",
    "    },\n",
    "    'SESRRelease_M5': {\n",
    "        'sesr_m5_2x': {\n",
    "            'scaling_factor': 2\n",
    "        },\n",
    "        'sesr_m5_3x': {\n",
    "            'scaling_factor': 3\n",
    "        },\n",
    "        'sesr_m5_4x': {\n",
    "            'scaling_factor': 4\n",
    "        }\n",
    "    },\n",
    "    'SESRRelease_M7': {\n",
    "        'sesr_m7_2x': {\n",
    "            'scaling_factor': 2\n",
    "        },\n",
    "        'sesr_m7_3x': {\n",
    "            'scaling_factor': 3\n",
    "        },\n",
    "        'sesr_m7_4x': {\n",
    "            'scaling_factor': 4\n",
    "        }\n",
    "    },\n",
    "    'SESRRelease_M11': {\n",
    "        'sesr_m11_2x': {\n",
    "            'scaling_factor': 2\n",
    "        },\n",
    "        'sesr_m11_3x': {\n",
    "            'scaling_factor': 3\n",
    "        },\n",
    "        'sesr_m11_4x': {\n",
    "            'scaling_factor': 4\n",
    "        }\n",
    "    },\n",
    "    'SESRRelease_XL': {\n",
    "         'sesr_xl_2x': {\n",
    "            'scaling_factor': 2\n",
    "        },\n",
    "        'sesr_xl_3x': {\n",
    "            'scaling_factor': 3\n",
    "        },\n",
    "        'sesr_xl_4x': {\n",
    "            'scaling_factor': 4\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "#NOTE: Set the following variable to the path where your checkpoint folders are located\n",
    "# (parent directory of the model checkpoint folder)\n",
    "CHECKPOINT_DIR = '<checkpoint_parent_folder>'\n",
    "#NOTE: Set the following variable to the path of your dataset (parent directory of actual images)\n",
    "DATA_DIR = '<root>/set5/SR_testing_datasets'\n",
    "\n",
    "FILENAME_FLOAT32 = 'checkpoint_float32.pth.tar'  # full precision model\n",
    "FILENAME_INT8 = 'checkpoint_int8.pth' # quantized model\n",
    "ENCODINGS = 'checkpoint_int8.encodings' # encodings of the quantized models\n",
    "\n",
    "MODELS = list(MODEL_ARGS.keys())\n",
    "\n",
    "DATASET_NAME = 'Set14'\n",
    "ENCODING_PATH = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c3706e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960a7a8",
   "metadata": {},
   "source": [
    "This is the model definition for the Anchor-based Plain Net (ABPN) model by Du et al. (https://arxiv.org/abs/2105.09750)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aabbbf6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ABPNRelease(nn.Module):\n",
    "    \"\"\"\n",
    "    Anchor-based Plain Net Model implementation (https://arxiv.org/abs/2105.09750).\n",
    "\n",
    "    2021 CVPR MAI SISR Winner -- Used quantization-aware training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 out_channels=3,\n",
    "                 num_channels=28,\n",
    "                 scaling_factor=2):\n",
    "        \"\"\"\n",
    "        :param in_channels:     number of channels for LR input (default 3 for RGB frames)\n",
    "        :param out_channels:    number of channels for HR output (default 3 for RGB frames)\n",
    "        :param num_channels:    number of feature channels for the convolutional layers (default 28 in paper)\n",
    "        :param scaling_factor:  scaling factor for LR-to-HR upscaling (default 2 for 4x upsampling)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=num_channels, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=out_channels * scaling_factor ** 2,\n",
    "                      kernel_size=(3, 3), padding=1)\n",
    "        )\n",
    "\n",
    "        self.anchor = blocks.AnchorOp(scaling_factor)  # (=channel-wise nearest upsampling)\n",
    "        self.add_residual = blocks.AddOp()\n",
    "        self.depth_to_space = nn.PixelShuffle(scaling_factor)\n",
    "\n",
    "    def forward(self, input):\n",
    "        residual = self.cnn(input)\n",
    "        upsampled_input = self.anchor(input)\n",
    "        output = self.add_residual(upsampled_input, residual)\n",
    "\n",
    "        return self.depth_to_space(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc977c",
   "metadata": {},
   "source": [
    "This is our implementation of the XLSR model proposed in \"Extremely Lightweight Quantization Robust Real-Time Single-Image Super Resolution\" by Ayazoglu et al. (https://arxiv.org/abs/2105.10288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLSRRelease(nn.Module):\n",
    "    \"\"\"\n",
    "    Extremely Lightweight Quantization Robust Real-Time Single-Image Super Resolution for Mobile Devices\n",
    "    by Ayazoglu et al. (https://arxiv.org/abs/2105.10288)\n",
    "    Official winner of Mobile AI 2021 Real-Time Single Image Super Resolution Challenge\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 out_channels=3,\n",
    "                 scaling_factor=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=(3, 3), padding=1)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1, 1), padding=0),\n",
    "            blocks.GBlock(in_channels=32),\n",
    "            blocks.GBlock(in_channels=32),\n",
    "            blocks.GBlock(in_channels=32)\n",
    "        )\n",
    "\n",
    "        self.concat_residual = blocks.ConcatOp()\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=48, out_channels=32, kernel_size=(1, 1), padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=out_channels * scaling_factor ** 2, kernel_size=(3, 3), padding=1)\n",
    "        )\n",
    "\n",
    "        self.depth_to_space = nn.PixelShuffle(scaling_factor)\n",
    "        self.clipped_relu = nn.Hardtanh(0, 1)  # Clipped ReLU\n",
    "\n",
    "    def forward(self, input):\n",
    "        residual = self.residual(input)\n",
    "        gblock_output = self.cnn(input)\n",
    "        concat_output = self.concat_residual(gblock_output, residual)\n",
    "        output = self.tail(concat_output)\n",
    "\n",
    "        return self.clipped_relu(self.depth_to_space(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d74204",
   "metadata": {},
   "source": [
    "This is our implementation of the Collapsible Linear Blocks for Super-Efficient Super Resolution (SESR) model proposed in Bhardwaj et al. (https://arxiv.org/abs/2103.09404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccceff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SESRRelease(nn.Module):\n",
    "    \"\"\"\n",
    "    Collapsible Linear Blocks for Super-Efficient Super Resolution, Bhardwaj et al. (https://arxiv.org/abs/2103.09404)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 out_channels=3,\n",
    "                 num_channels=16,\n",
    "                 num_lblocks=3,\n",
    "                 scaling_factor=2):\n",
    "        super().__init__()\n",
    "        self.anchor = blocks.AnchorOp(scaling_factor)  # (=channel-wise nearest upsampling)\n",
    "\n",
    "        self.conv_first = blocks.CollapsibleLinearBlock(in_channels=in_channels, out_channels=num_channels,\n",
    "                                                        tmp_channels=256, kernel_size=5, activation='relu')\n",
    "        \n",
    "        residual_layers = [\n",
    "            blocks.ResidualCollapsibleLinearBlock(in_channels=num_channels, out_channels=num_channels,\n",
    "                                                  tmp_channels=256, kernel_size=3, activation='relu')\n",
    "            for _ in range(num_lblocks)\n",
    "        ]\n",
    "        self.residual_block = nn.Sequential(*residual_layers)\n",
    "\n",
    "        self.add_residual = blocks.AddOp()\n",
    "        \n",
    "        self.conv_last = blocks.CollapsibleLinearBlock(in_channels=num_channels, \n",
    "                                                       out_channels=out_channels * scaling_factor ** 2,\n",
    "                                                       tmp_channels=256, kernel_size=5, activation='identity')\n",
    "\n",
    "        self.add_upsampled_input = blocks.AddOp()\n",
    "        self.depth_to_space = nn.PixelShuffle(scaling_factor)\n",
    "\n",
    "    def collapse(self):\n",
    "        self.conv_first.collapse()\n",
    "        for layer in self.residual_block:\n",
    "            layer.collapse()\n",
    "        self.conv_last.collapse()\n",
    "\n",
    "    def before_quantization(self):\n",
    "        self.collapse()\n",
    "\n",
    "    def forward(self, input):\n",
    "        upsampled_input = self.anchor(input)  # Get upsampled input from AnchorOp()\n",
    "        initial_features = self.conv_first(input)  # Extract features from conv-first\n",
    "        residual_features = self.residual_block(initial_features)  # Get residual features with `lblocks`\n",
    "        residual_features = self.add_residual(residual_features, initial_features)  # Add init_features and residual\n",
    "        final_features = self.conv_last(residual_features)  # Get final features from conv-last\n",
    "        output = self.add_upsampled_input(final_features, upsampled_input)  # Add final_features and upsampled_input\n",
    "\n",
    "        return self.depth_to_space(output)  # Depth-to-space and return\n",
    "\n",
    "\n",
    "class SESRRelease_M3(SESRRelease):\n",
    "    def __init__(self, scaling_factor, **kwargs):\n",
    "        super().__init__(scaling_factor=scaling_factor, num_lblocks=3, **kwargs)\n",
    "\n",
    "\n",
    "class SESRRelease_M5(SESRRelease):\n",
    "    def __init__(self, scaling_factor, **kwargs):\n",
    "        super().__init__(scaling_factor=scaling_factor, num_lblocks=5, **kwargs)\n",
    "\n",
    "\n",
    "class SESRRelease_M7(SESRRelease):\n",
    "    def __init__(self, scaling_factor, **kwargs):\n",
    "        super().__init__(scaling_factor=scaling_factor, num_lblocks=7, **kwargs)\n",
    "\n",
    "\n",
    "class SESRRelease_M11(SESRRelease):\n",
    "    def __init__(self, scaling_factor, **kwargs):\n",
    "        super().__init__(scaling_factor=scaling_factor, num_lblocks=11, **kwargs)\n",
    "\n",
    "\n",
    "class SESRRelease_XL(SESRRelease):\n",
    "    def __init__(self, scaling_factor, **kwargs):\n",
    "        super().__init__(scaling_factor=scaling_factor, num_channels=32, num_lblocks=11, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d3c2d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Helper methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b85a25",
   "metadata": {},
   "source": [
    "A bunch of functions to help us create the dataset, as well as evaluate average PSNR for all test-set images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e50353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(test_images_dir, scaling_factor=2):\n",
    "    \"\"\"\n",
    "    Load the images from the specified directory and develop the low-res and high-res images.\n",
    "    \n",
    "    :param test_images_dir:\n",
    "        Directory to get the test images from\n",
    "    :param scaling_factor:\n",
    "        Scaling factor to use while generating low-res images from their high-res counterparts\n",
    "    :return:\n",
    "        Pre-processed input images for the model, and low-res and high-res images for visualization\n",
    "    \"\"\"\n",
    "    # Input images for the model\n",
    "    INPUTS_LR = []\n",
    "\n",
    "    # Post-processed images for visualization\n",
    "    IMAGES_LR = []\n",
    "    IMAGES_HR = []\n",
    "\n",
    "    # Load the test images\n",
    "    for img_path in glob.glob(os.path.join(test_images_dir, '*')):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        lr_img, hr_img = preprocess(img, scaling_factor)\n",
    "        \n",
    "        INPUTS_LR.append(lr_img)\n",
    "        IMAGES_LR.append(post_process(lr_img))\n",
    "        IMAGES_HR.append(post_process(hr_img))\n",
    "\n",
    "    return INPUTS_LR, IMAGES_LR, IMAGES_HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d4b3c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_hr_lr_pair(img, scaling_factor=2):\n",
    "    \"\"\"\n",
    "    Create low-res images from high-res images.\n",
    "    \n",
    "    :param img:\n",
    "        The high-res image from which the low-res image is created\n",
    "    :param scaling_factor:\n",
    "         Scaling factor to use while generating low-res images\n",
    "    :return:\n",
    "        low-res and high-res image-pair\n",
    "    \"\"\"\n",
    "    height, width = img.shape[0:2]\n",
    "\n",
    "    # Take the largest possible center-crop of it such that its dimensions are perfectly divisible by the scaling factor\n",
    "    x_remainder = width % (scaling_factor)\n",
    "    y_remainder = height % (scaling_factor)\n",
    "    left = x_remainder // 2\n",
    "    top = y_remainder // 2\n",
    "    right = left + (width - x_remainder)\n",
    "    bottom = top + (height - y_remainder)\n",
    "    hr_img = img[top:bottom, left:right]\n",
    "\n",
    "    hr_height, hr_width = hr_img.shape[0:2]\n",
    "\n",
    "    hr_img = np.array(hr_img, dtype='float64')\n",
    "    lr_img = imresize(hr_img, 1. / scaling_factor)  # equivalent to matlab's imresize\n",
    "    lr_img = np.uint8(np.clip(lr_img, 0., 255.))  # this is to simulate matlab's imwrite operation\n",
    "    hr_img = np.uint8(hr_img)\n",
    "\n",
    "    lr_height, lr_width = lr_img.shape[0:2]\n",
    "\n",
    "    # Sanity check\n",
    "    assert hr_width == lr_width * scaling_factor and hr_height == lr_height * scaling_factor\n",
    "\n",
    "    lr_img = convert_image(lr_img, source='array', target='[0, 1]')\n",
    "    hr_img = convert_image(hr_img, source='array', target='[0, 1]')\n",
    "\n",
    "    return lr_img, hr_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152bde68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image(img, source, target):\n",
    "    \"\"\"\n",
    "    Convert image from numpy-array to float-tensor for torch.\n",
    "    \n",
    "    :param img:\n",
    "        Input image to perform conversion on\n",
    "    :param source:\n",
    "        The type of input image to be converted\n",
    "    :param target:\n",
    "        The type of image after conversion\n",
    "    :return:\n",
    "        The converted image from `source` to `target`\n",
    "    \"\"\"\n",
    "    if source == 'array':\n",
    "        img = torch.from_numpy(img.transpose((2, 0, 1))).contiguous()\n",
    "        img = img.to(dtype=torch.float32).div(255)\n",
    "    elif source == '[0, 1]':\n",
    "        img = torch.clamp(img, 0, 1)  # useful to post-process output of models that can overspill\n",
    "    \n",
    "    if target == '[0, 1]':\n",
    "        pass  # already in [0, 1]\n",
    "    elif target == 'y-channel':\n",
    "        # Based on definitions at https://github.com/xinntao/BasicSR/wiki/Color-conversion-in-SR\n",
    "        # torch.dot() does not work the same way as numpy.dot()\n",
    "        # So, use torch.matmul() to find the dot product between the last dimension of an 4-D tensor and a 1-D tensor\n",
    "        img = torch.matmul(img.permute(0, 2, 3, 1), RGB_WEIGHTS.to(img.device)) + 16.\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4923cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, scaling_factor=2):\n",
    "    \"\"\"\n",
    "    Generate low-res images from high-res inputs, downsample, and convert to tensors.\n",
    "    \n",
    "    :param img:\n",
    "        Input image to pre-process\n",
    "    :param scaling_factor:\n",
    "         Scaling factor to use while generating low-res and high-res image-pairs\n",
    "    :return:\n",
    "        Low-res and High-res image pairs after pre-processing\n",
    "    \"\"\"\n",
    "    lr_img, hr_img = create_hr_lr_pair(img, scaling_factor)\n",
    "\n",
    "    return lr_img, hr_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(img):\n",
    "    \"\"\"\n",
    "    Undo all preprocessing steps to get the upsampled low-res and high-res images.\n",
    "    \n",
    "    :param img:\n",
    "        The pre-processed image to be converted back for comparison\n",
    "    :return:\n",
    "        The image after reverting the changes done in the pre-processing steps\n",
    "    \"\"\"\n",
    "    img = img.detach().cpu().numpy()\n",
    "    img = np.clip(255. * img, 0., 255.)\n",
    "    img = np.uint8(img)\n",
    "    img = img.transpose(1, 2, 0)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d902068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image):\n",
    "    \"\"\"\n",
    "    Helper method to plot an image using PyPlot.\n",
    "    \n",
    "    :param image:\n",
    "        Image to plot\n",
    "    \"\"\"\n",
    "    plt.imshow(image, interpolation='nearest')\n",
    "    plt.tick_params(left=False,\n",
    "                    bottom=False,\n",
    "                    labelleft=False,\n",
    "                    labelbottom=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b056e8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_psnr(img_pred, img_true, data_range=255., eps=1e-8):\n",
    "    \"\"\"\n",
    "    Compute PSNR between super-resolved and original images.\n",
    "    \n",
    "    :param img_pred:\n",
    "        The super-resolved image obtained from the model\n",
    "    :param img_true:\n",
    "        The original high-res image\n",
    "    :param data_range:\n",
    "        Default = 255\n",
    "    :param eps:\n",
    "        Default = 1e-8\n",
    "    :return:\n",
    "        PSNR value\n",
    "    \"\"\"\n",
    "    err = (img_pred - img_true) ** 2\n",
    "    err = err.mean(dim=-1).mean(dim=-1)\n",
    "\n",
    "    return 10. * torch.log10((data_range ** 2) / (err + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68871ba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_psnr(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Evaluate individual PSNR metric for each super-res and actual high-res image-pair.\n",
    "    \n",
    "    :param y_pred:\n",
    "        The super-resolved image from the model\n",
    "    :param y_true:\n",
    "        The original high-res image\n",
    "    :return:\n",
    "        The evaluated PSNR metric for the image-pair\n",
    "    \"\"\"\n",
    "    y_pred = y_pred.transpose(2, 0, 1)[None] / 255.\n",
    "    y_true = y_true.transpose(2, 0, 1)[None] / 255.\n",
    "\n",
    "    sr_img = convert_image(torch.FloatTensor(y_pred),\n",
    "                           source='[0, 1]',\n",
    "                           target='y-channel')\n",
    "    hr_img = convert_image(torch.FloatTensor(y_true),\n",
    "                           source='[0, 1]',\n",
    "                           target='y-channel')\n",
    "\n",
    "    return compute_psnr(sr_img, hr_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_average_psnr(sr_images, hr_images):\n",
    "    \"\"\"\n",
    "    Evaluate the avg PSNR metric for all test-set super-res and high-res images.\n",
    "    \n",
    "    :param sr_images:\n",
    "        The list of super-resolved images obtained from the model for the given test-images\n",
    "    :param hr_images:\n",
    "        The list of original high-res test-images\n",
    "    :return:\n",
    "        Average PSNR metric for all super-resolved and high-res test-set image-pairs\n",
    "    \"\"\"\n",
    "    psnr = []\n",
    "    for sr_img, hr_img in zip(sr_images, hr_images):\n",
    "        psnr.append(evaluate_psnr(sr_img, hr_img))\n",
    "\n",
    "    average_psnr = np.mean(np.array(psnr))\n",
    "    \n",
    "    return average_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffee19a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_calibration_data(sim_model, calibration_data=None):\n",
    "    \"\"\"\n",
    "    Helper method to compute encodings for the QuantizationSimModel object.\n",
    "    \n",
    "    :param sim_model:\n",
    "        The QuantizationSimModel object to compute encodings for\n",
    "    :param calibration_data:\n",
    "        Tuple containing calibration images and a flag to use GPU or CPU\n",
    "    \"\"\"\n",
    "    (images_hr, use_cuda) = calibration_data\n",
    "    if use_cuda:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    sim_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img in images_hr:\n",
    "            lr_img, hr_img = preprocess(img, sim_model.scaling_factor)\n",
    "            input_img = lr_img.unsqueeze(0).to(device)\n",
    "            sim_model(input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51e89f3",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_checkpoint, model_name, model_args, use_quant_sim_model=False, \n",
    "               encoding_path='', calibration_data=None, use_cuda=True):\n",
    "    \"\"\"\n",
    "    Load model from checkpoint directory using the specified model arguments for the instance.\n",
    "    Optionally, you can use the QuantizationSimModel object to load the quantized model.\n",
    "    \n",
    "    :param model_checkpoint:\n",
    "        Path to model checkpoint to load the model weights from\n",
    "    :param model_name:\n",
    "        Name of the model as a string\n",
    "    :param model_args:\n",
    "        Set of arguments to use to create an instance of the model\n",
    "    :param use_quant_sim_model:\n",
    "        `True` if you want to use QuantizationSimModel, default: `False`\n",
    "    :param encoding_path:\n",
    "        Path to gather encodings for the quantized model\n",
    "    :param calibration_data:\n",
    "        Data to instantiate the QuantizationSimModel\n",
    "    :param use_cuda:\n",
    "        Use CUDA or CPU\n",
    "    :return:\n",
    "        One of the FP32-model or the quantized model\n",
    "    \"\"\"\n",
    "    if use_cuda:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    model = eval(model_name)(**model_args)\n",
    "    if use_quant_sim_model and hasattr(model, 'before_quantization'):\n",
    "        model.before_quantization()\n",
    "\n",
    "    print(f\"Loading model from checkpoint : {model_checkpoint}\")\n",
    "    state_dict = torch.load(model_checkpoint, map_location='cpu')['state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    \n",
    "    if use_quant_sim_model:\n",
    "        # Specify input-shape based on current model specification\n",
    "        dummy_input = torch.rand(1, 3, 512, 512).to(device)\n",
    "        \n",
    "        sim = QuantizationSimModel(model=model,\n",
    "                                   dummy_input=dummy_input,\n",
    "                                   quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                                   default_output_bw=8, \n",
    "                                   default_param_bw=8)\n",
    "\n",
    "        sim.set_and_freeze_param_encodings(encoding_path=encoding_path)\n",
    "\n",
    "        sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                              forward_pass_callback_args=(calibration_data, \n",
    "                                                          model_args['scaling_factor'], \n",
    "                                                          use_cuda))\n",
    "\n",
    "        return sim.model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea418402",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_model(model, inputs_lr, use_cuda):\n",
    "    \"\"\"\n",
    "    Run inference on the model with the set of given input test-images.\n",
    "    \n",
    "    :param model:\n",
    "        The model instance to infer from\n",
    "    :param INPUTS_LR:\n",
    "        The set of pre-processed input images to test\n",
    "    :param use_cuda:\n",
    "        Use CUDA or CPU\n",
    "    :return:\n",
    "        The super-resolved images obtained from the model for the given test-images\n",
    "    \"\"\"\n",
    "    if use_cuda:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    model.eval()\n",
    "    images_sr = []\n",
    "\n",
    "    # Inference\n",
    "    for count, img_lr in enumerate(inputs_lr):\n",
    "        with torch.no_grad():\n",
    "            sr_img = model(img_lr.unsqueeze(0).to(device)).squeeze(0)\n",
    "\n",
    "        images_sr.append(post_process(sr_img))\n",
    "    print('')\n",
    "\n",
    "    return images_sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2924fd4d",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b80ad5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualize(images_lr, images_hr, images_sr):\n",
    "    \"\"\"\n",
    "    Visualize test-images as low-res, super-res and high-res.\n",
    "    \n",
    "    :param images_lr:\n",
    "        List of low-res images\n",
    "    :param images_hr:\n",
    "        List of high-res images\n",
    "    :param images_sr:\n",
    "        List of super-resolved images\n",
    "    \"\"\"\n",
    "    num_images = len(images_lr)\n",
    "    plt.figure(figsize=(16, 4 * num_images))\n",
    "\n",
    "    count = 1\n",
    "    for lr_img, hr_img, sr_img in zip(images_lr, images_hr, images_sr):\n",
    "        # Sub-plot for Low-res images\n",
    "        plt.subplot(num_images, 3, count)\n",
    "        plt.title('LR')\n",
    "        imshow(lr_img)\n",
    "        count += 1\n",
    "\n",
    "        # Sub-plot for High-res images\n",
    "        plt.subplot(num_images, 3, count)\n",
    "        plt.title('HR')\n",
    "        imshow(hr_img)\n",
    "        count += 1\n",
    "\n",
    "        # Sub-plot for Super-res images\n",
    "        plt.subplot(num_images, 3, count)\n",
    "        plt.title('SR')\n",
    "        imshow(sr_img)\n",
    "        count += 1\n",
    "    \n",
    "    # Display results\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bcbe92",
   "metadata": {},
   "source": [
    "# Select a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33868e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DICT = {}\n",
    "for idx in range(len(MODELS)):\n",
    "    MODEL_DICT[idx] = MODELS[idx]\n",
    "\n",
    "MODEL_DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05484950",
   "metadata": {},
   "source": [
    "Select one of the models printed above by selecting the corresponding index in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = 0  # Model index\n",
    "\n",
    "MODEL_NAME = MODELS[model_index]  # Selected model type\n",
    "\n",
    "MODEL_SPECS = list(MODEL_ARGS.get(MODEL_NAME).keys())\n",
    "\n",
    "MODEL_SPECS_DICT = {}\n",
    "for idx in range(len(MODEL_SPECS)):\n",
    "    MODEL_SPECS_DICT[idx] = MODEL_SPECS[idx]\n",
    "\n",
    "MODEL_SPECS_DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913b88e8",
   "metadata": {},
   "source": [
    "Select one of the models printed above by selecting the corresponding index in the cell below and set `use_quantized` to `True` if you want to test the quantized model, else `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5388b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_index = 0  # Model specification index\n",
    "\n",
    "use_quantized = False  # Whether to use the INT8 quantized model or FP32 model\n",
    "use_cuda = True  # Whether to use CUDA or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model\n",
    "MODEL_CONFIG = MODEL_SPECS[model_spec_index]\n",
    "print(f'{MODEL_CONFIG} ({\"int8\" if use_quantized else \"float32\"}) will be used')\n",
    "\n",
    "# Path to desired model weights and encodings (if necessary)\n",
    "if use_quantized:\n",
    "    FILENAME = FILENAME_INT8\n",
    "    ENCODING_PATH = os.path.join(CHECKPOINT_DIR, f'release_{MODEL_CONFIG}', ENCODINGS)\n",
    "else:\n",
    "    FILENAME = FILENAME_FLOAT32\n",
    "\n",
    "# Path to model checkpoint and encodings (if necessary)\n",
    "MODEL_PATH = os.path.join(CHECKPOINT_DIR, f'release_{MODEL_CONFIG}', FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac4223",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94bb80b",
   "metadata": {},
   "source": [
    "Load test-set images (low-res and high-res pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c392f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to test images\n",
    "TEST_IMAGES_DIR = os.path.join(DATA_DIR, DATASET_NAME)\n",
    "\n",
    "# Get test images\n",
    "INPUTS_LR, IMAGES_LR, IMAGES_HR = load_dataset(TEST_IMAGES_DIR, MODEL_ARGS.get(MODEL_CONFIG)['scaling_factor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd1faa3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create model instance and load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca47bf9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = load_model(MODEL_PATH, MODEL_NAME, MODEL_ARGS.get(MODEL_CONFIG), \n",
    "                   use_quant_sim_model=use_quantized, encoding_path=ENCODING_PATH, \n",
    "                   calibration_data=IMAGES_HR if use_quantized else None,\n",
    "                   use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3783302e",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddda678",
   "metadata": {},
   "source": [
    "Run inference to get the respective super-resolved images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8819afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model inference on test images and get super-resolved images\n",
    "IMAGES_SR = run_model(model, INPUTS_LR, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93cc1c1",
   "metadata": {},
   "source": [
    "Calculate average-PSNR between the test-set high-res and super-resolved images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f63d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average PSNR for all test-images\n",
    "avg_psnr = evaluate_average_psnr(IMAGES_SR, IMAGES_HR)\n",
    "print(f\"\\n--- Average PSNR : {avg_psnr:.4f} ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ce5c8",
   "metadata": {},
   "source": [
    "Visualize the LR, HR and SR images side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a54c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize all test-images returned from the model\n",
    "visualize(IMAGES_LR, IMAGES_HR, IMAGES_SR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
